# PrivateLLM-Enchanted
Setup config for Colabs using Ngrok and tunneling it to Enchanted for use on remote devices so you don't need to download large files locally.  Works on iPhone as well.

Credit to Yanli Liu: The link to her walkthough is "[LINK] https://medium.com/towards-data-science/how-to-chat-with-any-open-source-llm-for-free-with-your-iphone-5fa7549efa9a"

Credit to TechXplainator: the link to their walkthough is "[LINK] https://github.com/TechXplainator/Tutorials/blob/main/ollama/ollama-on-colab/run-ollama-on-colab.ipynb"

I took the 2 sets of code and customize them to my needs and wants.  Feel free to alter as you wish.  Yanli's code allows you to make a change to the code and use the desired LLM Models from Ollama that you want.  Once added you simply hit the dropdown in Enchanted and select the model from there.  I uses the Colabs Pro version to get enhanced GPUs and storage for the models.  You can opt for TechXplantor's version that lets you run models from a Google Drive as well.(This code nor setup instructions go over that)

There is a Youtube video walkthrough for TechXplainator here: "[LINK] https://www.youtube.com/watch?v=9sPKTNGaPf8"

